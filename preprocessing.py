# -*- coding: utf-8 -*-
"""preProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pv7C-i3oJ7mH1RTdaJ4U3J0ShKL9awaf
"""

import re
import numpy as np
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer


def cleanText(text):
    text = str(text).lower()
    text = re.sub(r"i'm","i am",text)
    text = re.sub(r"he's","he is",text)
    text = re.sub(r"she's","she is",text)
    text = re.sub(r"that's","that is",text)
    text = re.sub(r"what's","what is",text)
    text = re.sub(r"where's","where is",text)
    text = re.sub(r"\'ll"," will",text)
    text = re.sub(r"\'ve"," have",text)
    text = re.sub(r"\'re"," are",text)
    text = re.sub(r"\'d"," would",text)
    text = re.sub(r"can't"," cannot",text)
    text = re.sub(r"couldn't", "could not", text)
    text = re.sub(r"wouldn't", "would not", text)
    text = re.sub(r"shouldn't", "should not", text)
    text = re.sub(r"shan't", "shall not", text)
    text = re.sub(r"won't"," will not",text)
    text = re.sub(r"don't","do not", text)
    text = re.sub(r"did't","did not", text)
    text = re.sub(r"aren't","are not", text)

    
    text = re.sub(r"[-\\()\"#/!*@;:\'<>{}+=|.?,]"," ",text)
    return text

dataset = pd.read_csv('train.csv')
dataset = dataset.drop(273514, axis=0)
textData = dataset.text.values
labelData = dataset.label.values



print(dataset.label.value_counts())


dataset['pre_clean_len'] = [len(t) for t in dataset.text]
count = 0
length = len(textData)
prev = -1
done_count = 0


pre_len = dataset.pre_clean_len.values

for i in labelData:
    done_count += 1
    done_percent = (int)(100*done_count/length)
    if done_percent != prev:
        prev = done_percent
        print("DONE = ", done_percent, "%.", sep='',flush=True)
    labelData[count] = int(i)
    count += 1
    


from pprint import pprint
data_dict = {
    'sentiment':{
        'type':dataset.label.dtype,
        'description':'sentiment class - 0:negative, 1:positive'
    },
    'text':{
        'type':dataset.text.dtype,
        'description':' text'
    },
    'pre_clean_len':{
        'type':dataset.pre_clean_len.dtype,
        'description':'Length of the text before cleaning'
    },
    'dataset_shape':dataset.shape
}
pprint(data_dict)

count = 0

from nltk import PorterStemmer

count = 0
stop = stopwords.words('english')
stop.remove('not')
stop.remove('against')
mystemmer = SnowballStemmer("english")
length = len(textData)
prev = -1
done_count = 0
import time as tm
startingTime = tm.time()
print("Starting Time is", startingTime)
for i in textData:
    
    done_count += 1
    done_percent = (int)(100*done_count/length)
    if done_percent != prev:
        prev = done_percent
        print("DONE = ", done_percent, "%.", sep='')
    
    textData[count] = cleanText(i)
    temp = textData[count].split()
    a = []
    for j in temp:
        if len(j) < 3:
            temp.remove(j)
            continue
        a.append(mystemmer.stem(str(j)))
        
    textData[count] = ' '.join(a)
    count += 1
endingTime = tm.time()
print("Time preprocessing took",endingTime - startingTime )

newdata = pd.DataFrame({'text':textData, 'labels':labelData})
newdata.index.name="index_no"  
newdata.to_csv("Text_DataFrame.csv")

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# # split into train and test
data_train, data_test, labels_train, labels_test = train_test_split(textData, labelData,test_size=0.05, random_state=0)

vectorizer = CountVectorizer()

data_train_count = vectorizer.fit_transform(data_train)
data_test_count  = vectorizer.transform(data_test)

print (data_train_count.shape, labels_train.shape, data_test_count.shape)

from sklearn import svm
clf = svm.SVC(decision_function_shape='ovo')
clf.fit(data_train_count.astype(str),labels_train.astype(str))
y_pred2 = clf.predict(data_test_count)

print(y_pred2)

from sklearn.metrics import confusion_matrix
cmLog = confusion_matrix(labels_test.astype('int32'),y_pred2.astype('int32'))
print(cmLog)

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(data_train_count.astype(str),labels_train.astype(str))

y_pred2 = classifier.predict(data_test_count)

print(y_pred2)

from sklearn.metrics import confusion_matrix
cmLog = confusion_matrix(labels_test.astype('int32'),y_pred2.astype('int32'))

for i in y_pred2:
    print(i)

from sklearn.tree import DecisionTreeClassifier
classifier2 = DecisionTreeClassifier(criterion = 'entropy', random_state =0, max_depth=35)

classifier2.fit(data_train_count.astype('int32'),labels_train.astype('int32'))

y_pred4 = classifier2.predict(data_test_count)
check4 = pd.DataFrame(y_pred4)

cmDT = confusion_matrix(labels_test.astype('int32'),y_pred4.astype('int32'))

print(cmDT)

from sklearn import metrics, cross_validation



